{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exploration\n",
    "\n",
    "### In this notebook we'll take a look at the 3 implementations and test them on 3 different examples & compare the results.\n",
    "\n",
    "First we'll import all 3 implementations"
   ],
   "id": "444ef7e04d3e452d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-26T12:27:43.478882Z",
     "start_time": "2025-10-26T12:27:43.474093Z"
    }
   },
   "source": [
    "from src.lexicon_absa import LexiconABSA\n",
    "from src.transformer_absa import ML_ABSA\n",
    "from src.llm_asba import LLMABSA"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we will initiate the models, and create our basic test, containing 3 samples, 1 Simple case, 1 Complex case, and 1 Edge case.",
   "id": "9f6cc5854c75c7b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T10:56:30.081145Z",
     "start_time": "2025-10-26T10:56:23.840945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models = [LexiconABSA(), ML_ABSA(), LLMABSA(), LLMABSA(\"mistral:7b\")]\n",
    "tests = [\n",
    "    \"The movie was a masterpiece — I almost fell asleep.\",\n",
    "    \"The camera quality is amazing but the battery life is awful.\",\n",
    "    \"The restaurant has a modern interior and the food is fine.\"\n",
    "]"
   ],
   "id": "d0f15328bdbe0c47",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bebef\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, iterate through our models and use them with our test sample.",
   "id": "54603208d1716bce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T10:57:22.283481Z",
     "start_time": "2025-10-26T10:56:33.357931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for model in models:\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Model: {model.name}\\n\")\n",
    "\n",
    "    for sample in tests:\n",
    "        print(f\"Sentence: {sample}\\n\")\n",
    "        results = model.analyze(sample)\n",
    "        if not results:\n",
    "            print(\"No aspects found!\\n\")\n",
    "        else:\n",
    "            for r in results:\n",
    "                print(f\" Aspect: {r.aspect:25} | Sentiment: {r.sentiment:8} | \"f\"Confidence: {r.confidence:.2f} | Span: {r.text_span}\")\n",
    "            print()"
   ],
   "id": "cf616996f72006d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Model: LexiconABSA\n",
      "\n",
      "Sentence: The movie was a masterpiece — I almost fell asleep.\n",
      "\n",
      "No aspects found!\n",
      "\n",
      "Sentence: The camera quality is amazing but the battery life is awful.\n",
      "\n",
      " Aspect: quality                   | Sentiment: positive | Confidence: 0.59 | Span: (11, 18)\n",
      " Aspect: life                      | Sentiment: negative | Confidence: 0.46 | Span: (46, 50)\n",
      "\n",
      "Sentence: The restaurant has a modern interior and the food is fine.\n",
      "\n",
      " Aspect: interior                  | Sentiment: neutral  | Confidence: 0.00 | Span: (28, 36)\n",
      " Aspect: food                      | Sentiment: positive | Confidence: 0.20 | Span: (45, 49)\n",
      "\n",
      "====================================================================================================\n",
      "Model: ML_ABSA\n",
      "\n",
      "Sentence: The movie was a masterpiece — I almost fell asleep.\n",
      "\n",
      " Aspect: The movie                 | Sentiment: positive | Confidence: 0.99 | Span: (0, 9)\n",
      " Aspect: a masterpiece             | Sentiment: positive | Confidence: 0.97 | Span: (14, 27)\n",
      "\n",
      "Sentence: The camera quality is amazing but the battery life is awful.\n",
      "\n",
      " Aspect: The camera quality        | Sentiment: positive | Confidence: 1.00 | Span: (0, 18)\n",
      " Aspect: the battery life          | Sentiment: negative | Confidence: 0.99 | Span: (34, 50)\n",
      "\n",
      "Sentence: The restaurant has a modern interior and the food is fine.\n",
      "\n",
      " Aspect: The restaurant            | Sentiment: neutral  | Confidence: 0.91 | Span: (0, 14)\n",
      " Aspect: a modern interior         | Sentiment: neutral  | Confidence: 0.81 | Span: (19, 36)\n",
      " Aspect: the food                  | Sentiment: positive | Confidence: 0.99 | Span: (41, 49)\n",
      "\n",
      "====================================================================================================\n",
      "Model: LLMABSA (phi3)\n",
      "\n",
      "Sentence: The movie was a masterpiece — I almost fell asleep.\n",
      "\n",
      " Aspect: movie                     | Sentiment: negative | Confidence: 0.95 | Span: (21, 32)\n",
      " Aspect: falling asleep            | Sentiment: positive | Confidence: 0.67 | Span: (48, 57)\n",
      "\n",
      "Sentence: The camera quality is amazing but the battery life is awful.\n",
      "\n",
      " Aspect: camera quality            | Sentiment: positive | Confidence: 0.92 | Span: (14, 28)\n",
      " Aspect: battery life              | Sentiment: negative | Confidence: 0.95 | Span: (36, 45)\n",
      "\n",
      "Sentence: The restaurant has a modern interior and the food is fine.\n",
      "\n",
      " Aspect: restaurant interior       | Sentiment: positive | Confidence: 0.90 | Span: (36, 47)\n",
      " Aspect: food quality              | Sentiment: neutral  | Confidence: 0.85 | Span: (128, 134)\n",
      "\n",
      "====================================================================================================\n",
      "Model: LLMABSA (mistral:7b)\n",
      "\n",
      "Sentence: The movie was a masterpiece — I almost fell asleep.\n",
      "\n",
      " Aspect: movie                     | Sentiment: positive | Confidence: 0.75 | Span: (4, 9)\n",
      " Aspect: sleeping                  | Sentiment: negative | Confidence: 0.85 | Span: (16, 27)\n",
      "\n",
      "Sentence: The camera quality is amazing but the battery life is awful.\n",
      "\n",
      " Aspect: camera quality            | Sentiment: positive | Confidence: 0.95 | Span: (4, 16)\n",
      " Aspect: battery life              | Sentiment: negative | Confidence: 0.90 | Span: (30, 38)\n",
      "\n",
      "Sentence: The restaurant has a modern interior and the food is fine.\n",
      "\n",
      " Aspect: restaurant interior       | Sentiment: positive | Confidence: 0.90 | Span: (4, 17)\n",
      " Aspect: food                      | Sentiment: neutral  | Confidence: 0.65 | Span: (23, 30)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "\n",
    "### Lexicon ASBA\n",
    "\n",
    "The LexiconABSA model uses a rule based approach, using spaCy to identify aspects and VADER for sentiment scoring.\n",
    "It identifies nouns and noun phrases as potential aspects and then looks for adjectives or verbs that modify them.\n",
    "It then calculates the sentiment score of those modifiers using VADER’s polarity scores, in order to determine whether the aspect is expressed positively, negatively, or neutral.\n",
    "\n",
    "Because it relies purely on lexical patterns and predefined sentiment scores, its understanding of language is literal. It performs best when opinions are clearly stated, for example “The battery is terrible” or “The food was amazing.”\n",
    "\n",
    "Looking at the results:\n",
    "\n",
    "- “The movie was a masterpiece — I almost fell asleep,” the model found no aspects. This is expected, since sarcasm is implicit, not expressed through negative words. VADER only reads direct sentiment terms, so it fails to interpret the irony that “masterpiece” is used sarcastically.\n",
    "\n",
    "- “The camera quality is amazing but the battery life is awful,” it correctly extracted quality (positive) and life (negative). However, it didn’t capture the full noun phrases (“camera quality”, “battery life”).\n",
    "\n",
    "- “The restaurant has a modern interior and the food is fine,” it identified interior and food, with food being positive(fine), but interior neutral, that's because adjectives like “modern” score near neutral in VADER’s lexicon."
   ],
   "id": "764d15f9507744e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ML ABSA\n",
    "\n",
    "The ML ABSA model uses a transformer based approach, using a pretrained model from Hugging Face.\n",
    "It uses spaCy to identify potential aspects in the text, and then pairs each with the full sentence to predict sentiment using the pretrained model.\n",
    "\n",
    "The model uses contextual embeddings to understand the relationship between aspects and opinions, allowing it to handle more complex grammar & sentences compared to the lexicon-based approach.\n",
    "But since it's a supervised classifier, it tends to interpret text literally and ends up suffering with sarcasm.\n",
    "\n",
    "Looking at the results:\n",
    "\n",
    "- “The movie was a masterpiece — I almost fell asleep,” both “The movie” and “a masterpiece” were labeled positive, showing that the model interprets sentiment directly from the positive adjectives and fails to detect sarcasm.\n",
    "\n",
    "- “The camera quality is amazing but the battery life is awful,” it performs perfectly, identifying both aspects and assigning the correct positive and negative sentiments with high confidence.\n",
    "\n",
    "- “The restaurant has a modern interior and the food is fine,” it correctly detected all aspects*(restaurant, interior, food), and gave the correct sentiment label for food(positive) but neutral for restaurant & interior, that's because no explicit opinion is expressed toward the restaurant itself, marking it as neutral. As for interior, the word \"modern\" carries a very weak sentiment polarity, just like with VADER.\n",
    "NOTE: The aspect detection limitation (detecting \"modern interior\" as an aspect) is due to design choice using noun chunking, and not the fault of the pretrained model."
   ],
   "id": "ecd32d63ff71a0cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LLM ASBA\n",
    "\n",
    "The LLM ABSA models use large language models(via Ollama) to perform aspect based sentiment analysis by giving them custom prompts.\n",
    "\n",
    "I’ve chosen two different LLMs:\n",
    "\n",
    "- Phi-3 (being the lightest one at ~2.2GB)\n",
    "- Mistral 7B (larger & more resource heavy at ~4.1GB)\n",
    "\n",
    "To compare how models with different sizes and reasoning capabilities respond to the same prompt.\n",
    "\n",
    "Looking at the results:\n",
    "\n",
    "“The movie was a masterpiece — I almost fell asleep,”\n",
    "\n",
    "Phi-3 managed to understand the sarcasm, correctly detecting the overall negative sentiment towards the movie, but marks falling asleep as positive, interpreting the literal action instead of the context(But then again there's nothing wrong with falling asleep, after all, everyone loves sleeping, so its positive!).\n",
    "\n",
    "Mistral on the other hand does the opposite, treating movie as positive and sleeping as negative.\n",
    "\n",
    "“The camera quality is amazing but the battery life is awful,” both models perform perfectly, extracting camera quality (positive) and battery life (negative) with strong confidence.\n",
    "\n",
    "“The restaurant has a modern interior and the food is fine,” both models correctly extract the aspects (interior and food) and assign the proper sentiments.\n",
    "\n",
    "With the main difference here being the confidence in food quality, Phi-3 rated the food quality as neutral with 0.85 confidence, and the Mistral at 0.65\n"
   ],
   "id": "87594b51e329ed62"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
